# -*- coding: utf-8 -*-
"""CoRef QC and Aggregation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14c0MYGfmfEkLN5YLlGNiM-uC9jX4zbPC
"""

!pip install allennlp
!pip install allennlp_models
!pip install -U spacy
!python -m spacy download en_core_web_sm
import spacy
from spacy.lang.en import English
from spacy.language import Language
from allennlp.predictors.predictor import Predictor
import allennlp_models.coref
import pandas as pd
import numpy as np

# adds columns to input csv to filter workers and returns new df with new columns
def add_filters(mturk_res):
  for col in mturk_res.columns:
    if col.startswith('Answer.deletion_mask_'):
      str_num = col[21:]
      num = int(str_num)
      if num < 2:
        mturk_res = mturk_res.rename(columns={"Answer.deletion_mask_1" : "Answer.sentence_1"})
      elif num == 2:
        mturk_res = mturk_res.rename(columns={"Answer.deletion_mask_2" : "Answer.pos_qual_ctrl_1"})
      elif num > 2 and num < 5:
        original_col = 'Answer.deletion_mask_' + str_num
        num = num - 1
        new_col = 'Answer.sentence_' + str(num)
        mturk_res = mturk_res.rename(columns={original_col : new_col})
      elif num == 5:
        mturk_res = mturk_res.rename(columns={'Answer.deletion_mask_5' : 'Answer.neg_qual_ctrl'})
      elif num > 5 and num < 9:
        original_col = 'Answer.deletion_mask_' + str_num
        num = num - 2
        new_col = 'Answer.sentence_' + str(num)
        mturk_res = mturk_res.rename(columns={original_col : new_col})
      elif num == 9:
        mturk_res = mturk_res.rename(columns={'Answer.deletion_mask_9' : 'Answer.pos_qual_ctrl_2'})
      else:
        original_col = 'Answer.deletion_mask_' + str_num
        num = num - 3
        new_col = 'Answer.sentence_' + str(num)
        mturk_res = mturk_res.rename(columns={original_col : new_col})
  mturk_res['time_spent'] = mturk_res['WorkTimeInSeconds'].apply(lambda x : x >= 60)
  mturk_res['neg_qual_ctrl_correct'] = mturk_res['Answer.neg_qual_ctrl'].apply(lambda x : x == '{}')
  mturk_res['pos_qual_ctrl_correct'] = mturk_res.apply(lambda x: True
             if  (isinstance(x['Answer.pos_qual_ctrl_1'], int)) &
                 (isinstance(x['Answer.pos_qual_ctrl_2'], int)) 
                 else False, axis = 1)
  mturk_res['num_clicks'] = 0
  for index in mturk_res.index:
    count = 0
    for col in mturk_res.columns:
      if col.startswith('Answer.sentence_'):
        if mturk_res[col][index] != '{}' and mturk_res[col][index] != '*':
          count += 1
    mturk_res['num_clicks'][index] = count
  mturk_res['filtered'] = mturk_res.apply(lambda x: True
                                          if (x['time_spent'] & x['neg_qual_ctrl_correct'] 
                                              & x['pos_qual_ctrl_correct'] & (x['num_clicks'] >= 5))
                                          else False, axis = 1)
  return mturk_res

# returns df with bad workers filtered out
def get_qualified_workers(mturk_res):
  updated = add_filters(mturk_res)
  return updated[updated['filtered']]

# returns list of bad workers
def get_bad_workers(mturk_res):
  updated = add_filters(mturk_res)
  bad_workers = []
  for index in updated.index:
    if not updated['filtered'][index]:
      bad_workers.append(updated['WorkerId'][index])
  return bad_workers

# The coreference resolution on the original text
def coref(original_text, predictor):
  resolved_text = predictor.coref_resolved(document=original_text)
  nlp = English()
  nlp.add_pipe('sentencizer')
  doc = nlp(resolved_text)
  resolved_sentences = [sent.text.strip() for sent in doc.sents]
  return resolved_sentences

# majority vote using only good workers
def majority_vote(article_HITs, coref_list): 
  label_list = []
  for col in article_HITs.columns:
    if col.startswith('Input.sentence_'):
      input_str = col
      str_num = col[15:]
      num = int(str_num) - 1
      answer_str = 'Answer.sentence_' + col[15:]
      keep = 0
      remove = 0
      for index, row in article_HITs.iterrows():
        if row[answer_str] == '{}':
          keep += 1
        elif row[answer_str] == '*':
          continue
        else: 
          remove += 1
      if keep > remove:
        label_list.append((coref_list[num], "keep", keep))
      elif keep == remove:
        rand_num = np.random.random()
        if rand_num >= 0.5:
          label_list.append((coref_list[num], "keep", keep))
        else: 
          label_list.append((coref_list[num], "remove", keep))
      else:
        label_list.append((coref_list[num], "remove", keep))
  return label_list

#AGGREGATION: aggregate the majority votes and send them to a new text file!
def vote_to_text(fileName, article_HITs):
  f = open(fileName, "w")
  for (index, row) in article_HITs.iterrows():
    if row['sentence'] == '*':
      continue
    elif row['label'] == 'keep':
      f.write(row['sentence'] + " ")
  f.close()

# Gets original text string by concat of columns 
def getText(mturk_res):
  text = ""
  for col in mturk_res.columns:
    if col.startswith('Input.sentence_'):
      str_num = col[15:]
      num = int(str_num)
      text = text + mturk_res.iloc[1][col] + " "
  return text

def main():
    # Read in CVS result file with pandas
    mturk_res = pd.read_csv('sample_QC_input.csv')
    original_text = getText(mturk_res)

    # Init CoRef Model
    predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz")

    # get list of bad workers based on updated df
    bad_workers = get_bad_workers(mturk_res)

    # get df with bad workers filtered out
    filtered = get_qualified_workers(mturk_res)

    # count votes based on filtered workers
    HIT_ids = filtered['HITId'].unique()
    for id in HIT_ids:
      #MAJORITY VOTING
      article_HITs = filtered.loc[filtered['HITId'] == id]
      coref_list = coref(original_text, predictor)
      label_list = majority_vote(article_HITs, coref_list)
      df = pd.DataFrame(label_list, columns=['sentence', 'label', 'keep_votes'])
      csv_name = 'output_' + id + '.csv' 
      df.to_csv(csv_name, index = False)
      #AGGREGATION
      vote_to_text(id, df) 
      
if __name__ == '__main__':
    main()